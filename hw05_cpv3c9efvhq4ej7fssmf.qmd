---
title: "Homework Assignment: Sentiment Analysis of Emma"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
library(janeaustenr)
library(tidytext)
library(gutenbergr)
library(wordcloud)
library(tidyverse)
```

**Title**: Exploring Joyful Language in Jane Austen’s *Emma* using Tidytext

**Objective**:

Use the **`tidytext`** package and three different **sentiment lexicons** (`nrc`, `afinn`, `bing`) to explore **positive/joyful words** in *Emma* by Jane Austen. You will tokenize the text, apply sentiment filters, visualize frequent sentiment words using `ggplot2`, and create a word cloud.

# **1. Data Preparation**

-   Load the `austen_books()` dataset from the **`janeaustenr`** package.
-   Group by book and detect chapter boundaries using regex.
-   Create `linenumber` and `chapter` columns.

```{r}
austen <- austen_books() %>% 
  group_by(book) %>% 
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(
      text,
      regex("^chapter [\\divxlc]",
        ignore_case = TRUE
      )
    )),
    .before = text
  ) %>% 
  ungroup() %>% 
  select(book, chapter, linenumber, text)
```

# **2. Tokenization**

-   Use `unnest_tokens()` to tokenize text into individual words.
-   Explain briefly **why we name the output column `word`** (include this as a comment in your script).

```{r}
tidy_austen <- austen %>% 
  unnest_tokens(word, text) %>%
  mutate(word = str_extract(word, "[a-z']+")) %>% #output column is named "word" because the tokenization is performed by unit of word, rather than other unit (e.g. sentence, paragraph, etc.)
  anti_join(stop_words, by = "word") ## filter out stop words

nrow(tidy_austen)
```

# **3. Sentiment Analysis**

## Filter joy/positive words from **each** of the three sentiment lexicons:

-   `nrc` (joy)
-   `afinn` (positive scores ≥ 1)
-   `bing` (positive)

```{r}

```

## Join each with *Emma*'s text and:

-   Count word frequency.
-   Filter for frequently occurring words (`n > 50`).
-   Visualize using a **bar chart** (`ggplot2`) and a **word cloud** (`wordcloud`).

```{r}

```

1.  **Push to GitHub**

    -   Push your complete R script (`.R` file) to a **GitHub repository**.

    -   Your script should include:

        -   Data wrangling
        -   Sentiment filtering and joins
        -   Visualization code

    -   You must include **at least 5 meaningful commits** to document your workflow.

------------------------------------------------------------------------

## **Rubric 100 Points**

| Category                                       | Points  |
|------------------------------------------------|---------|
| Step 1: Data wrangling (linenumber, chapter)   | 10      |
| Step 2: Tokenization + explanation             | 10      |
| Step 3a: NRC sentiment analysis + plots        | 10      |
| Step 3b: AFINN sentiment analysis + plots      | 10      |
| Step 3c: BING sentiment analysis + plots       | 10      |
| Code readability, structure, and comments      | 10      |
| Word cloud for each lexicon                    | 20      |
| GitHub submission                              |         |
| Commit history (at least 5 meaningful commits) | 20      |
| **Total**                                      | **100** |

Good Luck!
